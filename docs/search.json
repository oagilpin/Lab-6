[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "CSU_ESS330_Lab6",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggpubr)\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab6.html#q1-report-what-zero_q_freq-represents-zero_q_freq-represents-all-the-data-sources-and-their-descriptions-which-represent-the-percentage-days-that-there-were-zero-stream-flow-based-on-location.",
    "href": "lab6.html#q1-report-what-zero_q_freq-represents-zero_q_freq-represents-all-the-data-sources-and-their-descriptions-which-represent-the-percentage-days-that-there-were-zero-stream-flow-based-on-location.",
    "title": "CSU_ESS330_Lab6",
    "section": "Q1 Report what zero_q_freq represents: zero_q_freq represents all the data sources and their descriptions, which represent the percentage days that there were zero stream flow based on location.",
    "text": "Q1 Report what zero_q_freq represents: zero_q_freq represents all the data sources and their descriptions, which represent the percentage days that there were zero stream flow based on location."
  },
  {
    "objectID": "lab6.html#a.",
    "href": "lab6.html#a.",
    "title": "CSU_ESS330_Lab6",
    "section": "a.",
    "text": "a.\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nTest transformation:\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n\nBuild a recipe\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n\n\nFitting a linear model to the data\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCorrect version: prep -&gt; bake -&gt; predict\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\n\nModel Evaluation: statistical and visual\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\n\nUsing a workflow instead\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n\nMaking Predictions\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\n\nModel Evaluation: statistical and visual\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\nSwitch it up!\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\n\nPredictions\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\n\nModel Evaluation: statistical and visual\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.592\n2 rsq     standard       0.736\n3 mae     standard       0.367\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\nA workflowset approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     1\n2 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     1\n3 recipe_rand_fore… Prepro… rmse    0.565  0.0249    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.769  0.0261    10 recipe       rand…     2"
  },
  {
    "objectID": "lab6.html#q2-make-2-maps-of-the-sites-coloring-the-points-by-the-aridty-and-p_mean-column.-add-clear-labels-titles-and-a-color-scale-that-makes-sense-for-each-parameter.ensure-these-render-as-a-single-image-with-your-choice-of-facet_-patchwork-or-ggpubr.",
    "href": "lab6.html#q2-make-2-maps-of-the-sites-coloring-the-points-by-the-aridty-and-p_mean-column.-add-clear-labels-titles-and-a-color-scale-that-makes-sense-for-each-parameter.ensure-these-render-as-a-single-image-with-your-choice-of-facet_-patchwork-or-ggpubr.",
    "title": "CSU_ESS330_Lab6",
    "section": "Q2: Make 2 maps of the sites, coloring the points by the aridty and p_mean column. Add clear labels, titles, and a color scale that makes sense for each parameter.Ensure these render as a single image with your choice of facet_*, patchwork, or ggpubr.",
    "text": "Q2: Make 2 maps of the sites, coloring the points by the aridty and p_mean column. Add clear labels, titles, and a color scale that makes sense for each parameter.Ensure these render as a single image with your choice of facet_*, patchwork, or ggpubr.\n\np1 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"hotpink\", high = \"purple\") +\n  labs(x = \"Longitude\", y = \"Latitude\", title = \"Patterns of Aridity Across the United States\") + \n  ggthemes::theme_map()\n\n\np2 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"limegreen\", high = \"skyblue\") +\n  labs(x = \"Longitude\", y = \"Latitude\", title = \"U.S. Mean Daily Precipitation Patterns\") + \n  ggthemes::theme_map()\n\n\nggarrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nCheck if significant correlation between these variables\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\n\nVisual EDA\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nTo test a transformation, we can log transform the x and y axes using the scale_x_log10() and scale_y_log10() functions:\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nVisualize how a log transform may benifit the q_mean data\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nModel Building\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n\nPreprocessor: recipe`\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n\n\nNaive base lm approach:\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCorrect version: prep -&gt; bake -&gt; predict\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\n\nModel Evaluation: statistical and visual\n\n metrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\n\nUsing a workflow instead\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n\nMaking Predictions\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\n\nModel Evaluation: statistical and visual\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\nSwitch it up!\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\n\nPredictions\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\n\nModel Evaluation: statistical and visual\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.592\n2 rsq     standard       0.736\n3 mae     standard       0.367\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\nA workflowset approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     1\n2 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     1\n3 recipe_rand_fore… Prepro… rmse    0.565  0.0249    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.769  0.0261    10 recipe       rand…     2"
  },
  {
    "objectID": "lab6.html#q3build-a-xgboost-engine-regression-mode-model-using-boost_tree",
    "href": "lab6.html#q3build-a-xgboost-engine-regression-mode-model-using-boost_tree",
    "title": "CSU_ESS330_Lab6",
    "section": "Q3:Build a xgboost (engine) regression (mode) model using boost_tree",
    "text": "Q3:Build a xgboost (engine) regression (mode) model using boost_tree\n\nxgBoost_model &lt;- boost_tree(mode = \"regression\",\n                            trees = 1000) |&gt;\n  set_engine('xgboost')"
  },
  {
    "objectID": "lab6.html#q3build-a-neural-network-model-using-the-nnet-engine-from-the-baguette-package-using-the-bag_mlp-function",
    "href": "lab6.html#q3build-a-neural-network-model-using-the-nnet-engine-from-the-baguette-package-using-the-bag_mlp-function",
    "title": "CSU_ESS330_Lab6",
    "section": "Q3:Build a neural network model using the nnet engine from the baguette package using the bag_mlp function",
    "text": "Q3:Build a neural network model using the nnet engine from the baguette package using the bag_mlp function\n\nNeuralNet_Model &lt;- bag_mlp(mode = \"regression\") |&gt;\n  set_engine('nnet')"
  },
  {
    "objectID": "lab6.html#q3add-this-to-the-above-workflow",
    "href": "lab6.html#q3add-this-to-the-above-workflow",
    "title": "CSU_ESS330_Lab6",
    "section": "Q3:Add this to the above workflow",
    "text": "Q3:Add this to the above workflow\n\nxgbm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xgBoost_model) %&gt;%\n  fit(data = camels_train)\n\nNeuralNet_Model_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(NeuralNet_Model) %&gt;%\n  fit(data = camels_train)"
  },
  {
    "objectID": "lab6.html#q3evaluate-the-model-and-compare-it-to-the-linear-and-random-forest-models-the-boosted-tree-and-neural-network-models-produced-identical-results-while-the-outcomes-from-the-linear-regression-and-random-forest-models-showed-slight-discrepancies.-i-would-proceed-with-the-boosted-tree-and-neural-network-models-as-their-results-align-closely-with-the-11-line-and-the-metrics-are-statistically-significant.",
    "href": "lab6.html#q3evaluate-the-model-and-compare-it-to-the-linear-and-random-forest-models-the-boosted-tree-and-neural-network-models-produced-identical-results-while-the-outcomes-from-the-linear-regression-and-random-forest-models-showed-slight-discrepancies.-i-would-proceed-with-the-boosted-tree-and-neural-network-models-as-their-results-align-closely-with-the-11-line-and-the-metrics-are-statistically-significant.",
    "title": "CSU_ESS330_Lab6",
    "section": "Q3:Evaluate the model and compare it to the linear and random forest models: The boosted tree and neural network models produced identical results, while the outcomes from the linear regression and random forest models showed slight discrepancies. I would proceed with the boosted tree and neural network models, as their results align closely with the 1:1 line, and the metrics are statistically significant.",
    "text": "Q3:Evaluate the model and compare it to the linear and random forest models: The boosted tree and neural network models produced identical results, while the outcomes from the linear regression and random forest models showed slight discrepancies. I would proceed with the boosted tree and neural network models, as their results align closely with the 1:1 line, and the metrics are statistically significant.\n\nxgbm_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nrec &lt;- recipe(logQmean ~ p_mean + aridity + high_prec_dur, data = camels_train) %&gt;%\n  step_log(all_predictors())\n\nxgbm_wf &lt;- workflow() %&gt;%\n  add_model(xgbm_model) %&gt;%\n  add_recipe(rec)\n\n\ncamels_train_clean &lt;- camels_train %&gt;%\n  mutate(\n    p_mean = as.numeric(p_mean),\n    aridity = as.numeric(aridity),\n    high_prec_dur = as.numeric(high_prec_dur),\n    logQmean = as.numeric(logQmean)\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~ifelse(is.finite(.) & !is.na(.) & abs(.) &lt; 1e10, ., NA))) %&gt;%\n  drop_na()\n\nrec &lt;- recipe(logQmean ~ p_mean + aridity + high_prec_dur, data = camels_train_clean) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_normalize(all_predictors())  \n\nxgbm_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nxgbm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xgbm_model) %&gt;%\n  fit(data = camels_train_clean)\n\n\nNeuralNet_Model_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(NeuralNet_Model) %&gt;%\n  fit(data = camels_train_clean)\n\nxgb_predictions &lt;- augment(xgbm_wf, new_data = camels_train_clean)\nnn_predictions &lt;- augment(NeuralNet_Model_wf, new_data = camels_train_clean)\n\nlibrary(yardstick)\n\nxgb_metrics &lt;- metrics(xgb_predictions, truth = logQmean, estimate = .pred)\nprint(xgb_metrics)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      0.136 \n2 rsq     standard      0.990 \n3 mae     standard      0.0982\n\nnn_metrics &lt;- metrics(nn_predictions, truth = logQmean, estimate = .pred)\nprint(nn_metrics)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.443\n2 rsq     standard       0.876\n3 mae     standard       0.288\n\nggplot(xgb_predictions, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() +\n  labs(\n    title = \"Observed vs Predicted logQmean (XGBoost Model)\",\n    x = \"Observed logQmean\",\n    y = \"Predicted logQmean\",\n    colour = \"Aridity\"\n  )\n\n\n\n\n\n\n\nggplot(nn_predictions, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() +\n  labs(\n    title = \"Observed vs Predicted logQmean (Neural Network Model)\",\n    x = \"Observed logQmean\",\n    y = \"Predicted logQmean\",\n    colour = \"Aridity\"\n  )\n\n\n\n\n\n\n\nautoplot(wf)"
  },
  {
    "objectID": "lab6.html#q3which-of-the-4-models-would-you-move-forward-with-i-would-move-forward-with-the-boosted-tree-and-neural-network-models-because-their-results-align-closely-with-the-11-line-and-the-metrics-are-statistically-significant-which-indicates-accurate-and-reliable-predictions.",
    "href": "lab6.html#q3which-of-the-4-models-would-you-move-forward-with-i-would-move-forward-with-the-boosted-tree-and-neural-network-models-because-their-results-align-closely-with-the-11-line-and-the-metrics-are-statistically-significant-which-indicates-accurate-and-reliable-predictions.",
    "title": "CSU_ESS330_Lab6",
    "section": "Q3:Which of the 4 models would you move forward with? I would move forward with the boosted tree and neural network models because their results align closely with the 1:1 line and the metrics are statistically significant, which indicates accurate and reliable predictions.",
    "text": "Q3:Which of the 4 models would you move forward with? I would move forward with the boosted tree and neural network models because their results align closely with the 1:1 line and the metrics are statistically significant, which indicates accurate and reliable predictions."
  },
  {
    "objectID": "lab6.html#q4a-data-prep-data-splitting",
    "href": "lab6.html#q4a-data-prep-data-splitting",
    "title": "CSU_ESS330_Lab6",
    "section": "Q4a: Data Prep / Data Splitting",
    "text": "Q4a: Data Prep / Data Splitting\n\nset.seed(1234)\n\n\nresample_split &lt;- initial_split(camels, prop = 0.75)\n\n\ntrain_camels &lt;- training(resample_split)\nglimpse(train_camels)\n\nRows: 503\nColumns: 59\n$ gauge_id             &lt;chr&gt; \"04224775\", \"02038850\", \"12447390\", \"14138800\", \"…\n$ p_mean               &lt;dbl&gt; 2.914367, 3.149615, 2.559076, 7.729751, 3.617372,…\n$ pet_mean             &lt;dbl&gt; 2.244688, 2.889448, 2.188394, 2.094345, 2.804693,…\n$ p_seasonality        &lt;dbl&gt; 0.22762612, 0.05776050, -0.59836516, -0.81475942,…\n$ frac_snow            &lt;dbl&gt; 0.204650191, 0.065522598, 0.777311498, 0.31726621…\n$ aridity              &lt;dbl&gt; 0.7702146, 0.9173972, 0.8551500, 0.2709460, 0.775…\n$ high_prec_freq       &lt;dbl&gt; 17.20, 23.75, 18.95, 12.55, 23.75, 21.75, 23.95, …\n$ high_prec_dur        &lt;dbl&gt; 1.142857, 1.193467, 1.572614, 1.512048, 1.250000,…\n$ high_prec_timing     &lt;chr&gt; \"jja\", \"son\", \"djf\", \"djf\", \"mam\", \"mam\", \"jja\", …\n$ low_prec_freq        &lt;dbl&gt; 217.75, 267.65, 253.70, 200.85, 269.70, 260.55, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.151230, 4.979535, 5.720406, 5.641854, 5.272727,…\n$ low_prec_timing      &lt;chr&gt; \"jja\", \"son\", \"jja\", \"jja\", \"son\", \"son\", \"son\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Siliciclastic…\n$ glim_1st_class_frac  &lt;dbl&gt; 1.0000000, 0.7783589, 1.0000000, 0.7408656, 0.656…\n$ geol_2nd_class       &lt;chr&gt; NA, \"Metamorphics\", NA, \"Unconsolidated sediments…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.00000000, 0.15483860, 0.00000000, 0.25913437, 0…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.00000000…\n$ geol_porostiy        &lt;dbl&gt; 0.1338, 0.0996, 0.0100, 0.1392, 0.1580, 0.0128, 0…\n$ geol_permeability    &lt;dbl&gt; -16.2436, -15.8882, -14.1000, -12.0854, -14.0789,…\n$ soil_depth_pelletier &lt;dbl&gt; 4.0527704, 1.0270270, 0.4727273, 0.8974359, 1.048…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.1054176, 1.5000000, 1.5000000, 0.8373043, 1.424…\n$ soil_porosity        &lt;dbl&gt; 0.4642713, 0.4560548, 0.4344597, 0.4431248, 0.450…\n$ soil_conductivity    &lt;dbl&gt; 0.9744709, 0.5862081, 1.8269572, 1.5541585, 0.892…\n$ max_water_content    &lt;dbl&gt; 0.4783560, 0.7018239, 0.6421171, 0.3598810, 0.510…\n$ sand_frac            &lt;dbl&gt; 22.77199, 22.18412, 45.32336, 39.28517, 22.49397,…\n$ silt_frac            &lt;dbl&gt; 56.43728, 30.80408, 37.98377, 44.37471, 31.19531,…\n$ clay_frac            &lt;dbl&gt; 16.719066, 47.145138, 16.705685, 16.460240, 24.23…\n$ water_frac           &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ other_frac           &lt;dbl&gt; 4.0893952, 0.0000000, 0.0000000, 0.0000000, 22.07…\n$ gauge_lat            &lt;dbl&gt; 42.53562, 37.41542, 48.82292, 45.45262, 35.98318,…\n$ gauge_lon            &lt;dbl&gt; -77.70416, -78.63584, -120.14592, -121.89147, -92…\n$ elev_mean            &lt;dbl&gt; 519.45, 176.41, 1700.98, 821.62, 459.08, 387.44, …\n$ slope_mean           &lt;dbl&gt; 36.93333, 9.80131, 172.58264, 142.23871, 41.94700…\n$ area_gages2          &lt;dbl&gt; 231.84, 22.00, 58.10, 21.29, 2149.36, 240.77, 144…\n$ area_geospa_fabric   &lt;dbl&gt; 232.59, 23.80, 58.40, 21.83, 2147.73, 241.31, 145…\n$ frac_forest          &lt;dbl&gt; 0.8681, 0.9300, 0.9643, 1.0000, 0.8844, 0.9407, 0…\n$ lai_max              &lt;dbl&gt; 4.3014541, 5.3991196, 1.1999529, 4.0763384, 4.958…\n$ lai_diff             &lt;dbl&gt; 3.7519191, 4.3350245, 0.7890515, 2.3938173, 4.398…\n$ gvf_max              &lt;dbl&gt; 0.8525770, 0.8844151, 0.4894325, 0.8705525, 0.870…\n$ gvf_diff             &lt;dbl&gt; 0.4975784, 0.3792956, 0.1804294, 0.2019336, 0.491…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.5212560, 0.6610673, 0.8600801, 1.0000000, 0.895…\n$ dom_land_cover       &lt;chr&gt; \"    Deciduous Broadleaf Forest\", \"    Mixed Fore…\n$ root_depth_50        &lt;dbl&gt; 0.1852126, 0.2296640, 0.1630040, 0.1700000, 0.188…\n$ root_depth_99        &lt;dbl&gt; 1.760628, 2.264427, 1.758024, 1.800000, 1.947805,…\n$ q_mean               &lt;dbl&gt; 1.12964683, 0.88795895, 1.36554402, 6.51073552, 1…\n$ runoff_ratio         &lt;dbl&gt; 0.387613119, 0.281926158, 0.533608238, 0.84229565…\n$ slope_fdc            &lt;dbl&gt; 1.6984974, 1.2903515, 0.8783967, 1.8185192, 1.670…\n$ baseflow_index       &lt;dbl&gt; 0.506242202, 0.580267146, 0.534019978, 0.45786958…\n$ stream_elas          &lt;dbl&gt; 0.02287331, 2.18611483, 0.86605745, 1.09287074, 2…\n$ q5                   &lt;dbl&gt; 0.1055286208, 0.0814042409, 0.1221182286, 0.25281…\n$ q95                  &lt;dbl&gt; 3.69350173, 2.55778352, 7.10812310, 23.55791390, …\n$ high_q_freq          &lt;dbl&gt; 11.250000, 6.650000, 48.800000, 12.800000, 24.000…\n$ high_q_dur           &lt;dbl&gt; 1.906780, 1.445652, 26.378378, 1.910448, 3.380282…\n$ low_q_freq           &lt;dbl&gt; 79.15000, 43.15000, 146.50000, 107.65000, 152.800…\n$ low_q_dur            &lt;dbl&gt; 10.993056, 12.507246, 33.295455, 24.191011, 32.86…\n$ zero_q_freq          &lt;dbl&gt; 0.0000000000, 0.0000000000, 0.0000000000, 0.00000…\n$ hfd_mean             &lt;dbl&gt; 164.4000, 165.3500, 241.3000, 135.8500, 167.8000,…\n$ logQmean             &lt;dbl&gt; 0.12190505, -0.11882976, 0.31155290, 1.87345243, …\n\ntest_camels &lt;- testing(resample_split)\nglimpse(test_camels)\n\nRows: 168\nColumns: 59\n$ gauge_id             &lt;chr&gt; \"01030500\", \"01047000\", \"01052500\", \"01055000\", \"…\n$ p_mean               &lt;dbl&gt; 3.274405, 3.323146, 3.730858, 3.494183, 3.570500,…\n$ pet_mean             &lt;dbl&gt; 2.043594, 2.090024, 2.096423, 2.093235, 2.132744,…\n$ p_seasonality        &lt;dbl&gt; 0.047358189, 0.147775614, 0.152096803, 0.16722904…\n$ frac_snow            &lt;dbl&gt; 0.27701840, 0.28011813, 0.35269825, 0.30604885, 0…\n$ aridity              &lt;dbl&gt; 0.6241114, 0.6289293, 0.5619145, 0.5990628, 0.597…\n$ high_prec_freq       &lt;dbl&gt; 17.15, 20.10, 13.50, 19.15, 20.35, 15.50, 18.65, …\n$ high_prec_dur        &lt;dbl&gt; 1.207746, 1.165217, 1.129707, 1.167683, 1.169540,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"jja\", \"son\", \"son\", \"son\", \"jja\", …\n$ low_prec_freq        &lt;dbl&gt; 215.60, 235.90, 193.50, 224.85, 239.30, 204.35, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.514262, 3.691706, 2.896707, 3.378663, 3.747847,…\n$ low_prec_timing      &lt;chr&gt; \"djf\", \"djf\", \"mam\", \"mam\", \"djf\", \"mam\", \"mam\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Metamorphics\"…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.5733054, 0.3084880, 0.4974583, 0.6808434, 0.451…\n$ geol_2nd_class       &lt;chr&gt; \"Metamorphics\", \"Acid plutonic rocks\", \"Metamorph…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.2870100056, 0.2886125463, 0.3740616968, 0.16836…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.052140094, 0.000000000, 0.000000000, 0.00000000…\n$ geol_porostiy        &lt;dbl&gt; 0.1178, 0.0522, 0.0711, 0.1455, 0.0251, 0.0100, 0…\n$ geol_permeability    &lt;dbl&gt; -14.4918, -14.4819, -15.1658, -14.3578, -13.9903,…\n$ soil_depth_pelletier &lt;dbl&gt; 19.0114144, 5.3596549, 1.3013493, 2.1270588, 3.75…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.4613632, 1.3927785, 1.4948074, 1.3696566, 1.500…\n$ soil_porosity        &lt;dbl&gt; 0.4590910, 0.4227490, 0.4523257, 0.4240259, 0.404…\n$ soil_conductivity    &lt;dbl&gt; 1.289807, 2.615154, 1.262995, 2.550001, 4.030392,…\n$ max_water_content    &lt;dbl&gt; 0.6530198, 0.5611808, 0.6155380, 0.5470206, 0.617…\n$ sand_frac            &lt;dbl&gt; 32.23546, 55.16313, 30.55767, 53.86201, 69.00674,…\n$ silt_frac            &lt;dbl&gt; 51.77918, 34.18544, 52.61465, 34.73449, 22.94329,…\n$ clay_frac            &lt;dbl&gt; 14.776824, 10.303622, 11.143326, 10.346323, 7.817…\n$ water_frac           &lt;dbl&gt; 1.63434486, 0.00000000, 0.00000000, 0.00000000, 0…\n$ organic_frac         &lt;dbl&gt; 1.330278, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ other_frac           &lt;dbl&gt; 0.0220161, 0.1478672, 5.6755267, 0.7780162, 0.000…\n$ gauge_lat            &lt;dbl&gt; 45.50097, 44.86920, 44.87739, 44.64275, 44.30399,…\n$ gauge_lon            &lt;dbl&gt; -68.30596, -69.95510, -71.05749, -70.58878, -70.5…\n$ elev_mean            &lt;dbl&gt; 143.80, 310.38, 615.70, 423.12, 215.59, 784.85, 3…\n$ slope_mean           &lt;dbl&gt; 12.79195, 49.92122, 60.05183, 72.81304, 32.68445,…\n$ area_gages2          &lt;dbl&gt; 3676.17, 909.10, 383.82, 250.64, 190.92, 228.55, …\n$ area_geospa_fabric   &lt;dbl&gt; 3676.09, 904.94, 396.10, 251.16, 197.70, 238.32, …\n$ frac_forest          &lt;dbl&gt; 0.8782, 0.9906, 1.0000, 0.9916, 0.9415, 0.9972, 0…\n$ lai_max              &lt;dbl&gt; 4.685200, 5.086811, 4.800830, 5.030033, 5.362949,…\n$ lai_diff             &lt;dbl&gt; 3.665543, 4.300978, 4.124313, 4.319226, 4.587077,…\n$ gvf_max              &lt;dbl&gt; 0.8585020, 0.8913828, 0.8800341, 0.8861635, 0.905…\n$ gvf_diff             &lt;dbl&gt; 0.3513934, 0.4454732, 0.4773279, 0.4714315, 0.458…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.9752580, 0.8504500, 0.5935884, 0.5242488, 0.532…\n$ dom_land_cover       &lt;chr&gt; \"    Mixed Forests\", \"    Mixed Forests\", \"    Mi…\n$ root_depth_50        &lt;dbl&gt; NA, 0.2410270, 0.2256153, 0.2214549, 0.2091862, 0…\n$ root_depth_99        &lt;dbl&gt; NA, 2.340180, 2.237435, 2.209700, 2.073141, 2.364…\n$ q_mean               &lt;dbl&gt; 1.8201075, 2.1828696, 2.4051046, 2.2798975, 1.823…\n$ runoff_ratio         &lt;dbl&gt; 0.5558590, 0.6568684, 0.6446518, 0.6524836, 0.510…\n$ slope_fdc            &lt;dbl&gt; 1.871110, 1.415939, 1.301062, 1.349312, 1.533232,…\n$ baseflow_index       &lt;dbl&gt; 0.5084407, 0.4734649, 0.4596999, 0.4381317, 0.474…\n$ stream_elas          &lt;dbl&gt; 1.3775052, 1.5102375, 1.0255555, 1.3665871, 1.280…\n$ q5                   &lt;dbl&gt; 0.10714920, 0.19645805, 0.30596536, 0.18546495, 0…\n$ q95                  &lt;dbl&gt; 6.854887, 8.095148, 8.669019, 8.441584, 6.866097,…\n$ high_q_freq          &lt;dbl&gt; 12.25, 14.95, 14.10, 16.25, 13.85, 8.25, 4.15, 6.…\n$ high_q_dur           &lt;dbl&gt; 7.205882, 2.577586, 2.517857, 2.056962, 2.429825,…\n$ low_q_freq           &lt;dbl&gt; 89.25, 71.55, 58.90, 83.60, 82.95, 17.30, 35.15, …\n$ low_q_dur            &lt;dbl&gt; 19.402174, 12.776786, 7.316770, 8.941176, 13.8250…\n$ zero_q_freq          &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 184.90, 184.80, 197.20, 186.05, 178.05, 189.15, 1…\n$ logQmean             &lt;dbl&gt; 0.598895589, 0.780640330, 0.877593397, 0.82413047…\n\n\n\ncv_folds &lt;- vfold_cv(train_camels, v = 10)\n\ncv_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [452/51]&gt; Fold01\n 2 &lt;split [452/51]&gt; Fold02\n 3 &lt;split [452/51]&gt; Fold03\n 4 &lt;split [453/50]&gt; Fold04\n 5 &lt;split [453/50]&gt; Fold05\n 6 &lt;split [453/50]&gt; Fold06\n 7 &lt;split [453/50]&gt; Fold07\n 8 &lt;split [453/50]&gt; Fold08\n 9 &lt;split [453/50]&gt; Fold09\n10 &lt;split [453/50]&gt; Fold10"
  },
  {
    "objectID": "lab6.html#q4b-recipe",
    "href": "lab6.html#q4b-recipe",
    "title": "CSU_ESS330_Lab6",
    "section": "Q4b: Recipe",
    "text": "Q4b: Recipe\n\nformula &lt;- logQmean ~ p_mean + aridity + high_prec_dur\n\n\nDescribe in words why you are choosing the formula you are. Consult the downloaded PDF for the data to help you make this decision: I chose the predictor variables p_mean, aridity, and logQmean for the formula because they are likely key factors affecting mean daily discharge. Precipitation contributes water to the system, directly influencing discharge. Aridity, which indicates the dryness of the environment, is expected to result in lower logQmean in more arid areas. Additionally, I believe that high_prec_dur will be positively correlated with logQmean, as an increase in high precipitation events tends to raise the mean daily discharge.\n\ntrain_camels &lt;- na.omit(train_camels)\n\n\nrec &lt;- recipe(logQmean ~ p_mean + aridity + high_prec_dur, data = train_camels) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  prep(training = train_camels, retain = TRUE)\n\n\nrec &lt;- recipe(logQmean ~ p_mean + aridity + high_prec_dur, data = train_camels) %&gt;%\n  step_naomit(all_predictors(), all_outcomes()) %&gt;%  \n  step_zv(all_predictors())  \n\n\nrec_prep &lt;- prep(rec, training = train_camels)\nbaked_data &lt;- bake(rec_prep, new_data = NULL)\n\n\nsum(is.na(baked_data)) \n\n[1] 0\n\nsum(is.infinite(as.matrix(baked_data))) \n\n[1] 0"
  },
  {
    "objectID": "lab6.html#q4c-define-3-models-define-a-random-forest-model-using-the-rand_forest-function-and-set-the-engine-to-ranger-and-the-mode-to-regression",
    "href": "lab6.html#q4c-define-3-models-define-a-random-forest-model-using-the-rand_forest-function-and-set-the-engine-to-ranger-and-the-mode-to-regression",
    "title": "CSU_ESS330_Lab6",
    "section": "Q4c: Define 3 models ###Define a random forest model using the rand_forest function and set the engine to ranger and the mode to regression",
    "text": "Q4c: Define 3 models ###Define a random forest model using the rand_forest function and set the engine to ranger and the mode to regression\n\nQ4_rf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n\nDefine two other models of your choice\n\nQ4_lm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\nQ4_gbm_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")"
  },
  {
    "objectID": "lab6.html#q4d-workflow-set",
    "href": "lab6.html#q4d-workflow-set",
    "title": "CSU_ESS330_Lab6",
    "section": "Q4d: workflow set",
    "text": "Q4d: workflow set\n\nQ4_rf_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(Q4_rf_model)\n\nQ4_lm_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(Q4_lm_model)\n\nQ4_gbm_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(Q4_gbm_model)\n\n\nrf_results &lt;- fit_resamples(Q4_rf_wf, resamples = cv_folds)\nlm_results &lt;- fit_resamples(Q4_lm_wf, resamples = cv_folds)\ngbm_results &lt;- fit_resamples(Q4_gbm_wf, resamples = cv_folds)"
  },
  {
    "objectID": "lab6.html#q4e-evaluation",
    "href": "lab6.html#q4e-evaluation",
    "title": "CSU_ESS330_Lab6",
    "section": "Q4e: Evaluation",
    "text": "Q4e: Evaluation\n\nwf &lt;- workflow_set(list(rec), list(Q4_rf_model, Q4_lm_model, Q4_gbm_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.533  0.0292    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.792  0.0262    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.549  0.0281    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.780  0.0248    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    0.621  0.0318    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.731  0.0241    10 recipe       line…     3\n\n\n\nDescribe what model you think is best and why! After looking at the evaluation results, I’d probably go with either the Random Forest or Gradient Boosting model, due to the R-squared values. Both of these models are better at handling complex, non-linear relationships in the data, making them more reliable than the simpler Linear Regression. If one of the ensemble models stands out, it’d be the clear winner for its ability to capture more detailed patterns."
  },
  {
    "objectID": "lab6.html#q4f-extract-and-evaluate",
    "href": "lab6.html#q4f-extract-and-evaluate",
    "title": "CSU_ESS330_Lab6",
    "section": "Q4f: Extract and Evaluate",
    "text": "Q4f: Extract and Evaluate\n\nfinal_workflow &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(Q4_rf_model) |&gt;\n  fit(data = train_camels)\n\n\nfinal_workflow_data &lt;- augment(final_workflow, new_data = camels_test)\n\n\nggplot(final_workflow_data, aes(x = .pred, y = logQmean, colour = logQmean)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"darkred\", linetype = \"dashed\") +\n  labs(title = \"Observed vs. Predicted Values\",\n       x = \"Predicted logQmean\",\n       y = \"Observed logQmean\") +\n  scale_color_viridis_c()\n\n\n\n\n\n\n\n\n\nDescribe what you think of the results! The results are quite accurate, with the observed versus predicted logQmean values closely following the 1:1 line, indicating strong predictive performance. This suggests that the model is effectively capturing the relationship between the selected predictors and logQmean."
  }
]